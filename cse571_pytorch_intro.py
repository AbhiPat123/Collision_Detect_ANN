# -*- coding: utf-8 -*-
"""CSE571_PyTorch_Intro.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13CYvOsNN2WB13hu54YL0Ogja9JlsJ9N4
"""

import torch
import torch.nn as nn

"""# PyTorch

BUILD A NETWORK / MODEL<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- initialize network layers<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- define function to compute forward pass<br/>
"""

# a basic NN
class Basic_Network(nn.Module):
    # initialize with a set of inputs, hidden and output nodes - Define Architecture
    def __init__(self, input_size=5, hidden_size=4, output_size=2):
        # initializing the base (super) class of this class - same as super().__init__()
        super(Basic_Network, self).__init__()
        # relationship between input and hidden layer - Linear Layer
        self.input_to_hidden = nn.Linear(input_size, hidden_size)
        # relationship between hidden and output layer - Linear Layer
        self.hidden_to_output = nn.Linear(hidden_size, output_size)
        # non-linear activation to use
        self.nonlinear_activation = nn.Sigmoid()

    # the forward pass from inputs to outputs - Define computation
    def forward(self, network_input):
        # take the network input and pass through hidden layer (no activation yet)
        hidden = self.input_to_hidden(network_inpt)
        # apply nonlinear activation on the previous computation
        hidden = self.nonlinear_activation(hidden)
        # pass the output of hidden layer through the final/output layer
        network_output = self.hidden_to_output(hidden)
        # return the network output value
        return network_output

"""&nbsp;&nbsp;&nbsp;&nbsp;- build a basic_model object for network<br/>"""

# create a model with default numbers of neurons for our Basic Network
basic_model = Basic_Network()

"""&nbsp;&nbsp;&nbsp;&nbsp;- perform forward pass on the model<br/>"""

# compute networkoutput for a specific input
network_input = 
network_output = basic_model(network_input)

"""TRAIN MODEL<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- define a learning rate, optimizer and loss function<br/>
"""

# set a learning rate
learning_rate = 0.01

# set an optimizer to use - pass the basic_model.parameters()
optimizer = torch.optim.SGD(basic_model.parameters(), lr=learning_rate)

# Loss Function takes (output, target) pair and finds loss for those values
loss_function = nn.MSELoss()

"""&nbsp;&nbsp;&nbsp;&nbsp;- reset to zero for the parameters' gradients<br/>"""

# zero out all parameters' gradient values - before performing every step of backward
# NOTE: this is important because the backward() function acumulates gradients and does not overwrite it
optimizer.zero_grad()

# recomputing forard pass here (to keep consistency in steps performed)
network_output = basic_model(network_input)

"""&nbsp;&nbsp;&nbsp;&nbsp;- compute loss<br/>"""

# compute the loss between two sets of values - outputs and targets
loss = loss_function(network_output, target_output)
# print the loss value
print(loss.item())

"""&nbsp;&nbsp;&nbsp;&nbsp;- perform backward on the loss<br/>"""

# run one step of the BP Algorthm
# this accummulates the gradients value of each parameter in the parameters' .grad field
loss.backward()

"""&nbsp;&nbsp;&nbsp;&nbsp;- use optimizer's step function to update paramters<br/>"""

# run one step of optimizer (one step of Gradient Descent) - actually updates the parameters
optimizer.step()